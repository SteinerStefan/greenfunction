%anderer TITEL?
\section{Algorithmus}
	Um eine Visualisierung überhaupt durchführen zu können, braucht es in unserem Fall Daten, mit denen wir arbeiten können. Diese liefert uns ein Programm, bei welchem wir Daten einspeisen und im Gegenzug bearbeitete erhalten. 
	
	Das Problem, welches wir hier lösen wurde ebenfalls von Philip Solenthaler und Reto Christen bearbeitet. Sie haben untersucht, wie ein Programm geschrieben werden kann, welches dieses Problem möglichst effizient löst.
	\subsection{Mathematischer Ansatz zur Lösung}
%	\begin{shadedSmaller}
%		Der Algorithmus sollte uns die Werte liefern.
%		Verweis auf die Arbeit von Christen/Solenthaler		
%		Parallelisierung: Wieso OpenMP?		
%		Berechnung von einem Step
%	\end{shadedSmaller}	
	Das Potentialproblem auf der leitenden Platte, welches wir lösen möchten, hat nun folgende partielle Differentialgleichung. 
	\begin{equation}\label{eq:gleichung}
		\dfrac{\partial^2 u}{\partial x^2}+\dfrac{\partial^2 u}{\partial y^2} = f(x,y).
	\end{equation}
	Wobei die Rand-, und Anfangsbedingungen auf der leitenden Platte alle null sind. 
	
	Die Platte wird dabei diskretisiert. Wir bekommen eine endliche Anzahl an Punkten. Dabei wird für jeden Punkt diese Gleichung gelöst. Der Ansatz für eine Lösung dieses Problems bietet die lineare Algebra. Welche bereits im Theorie teil des Skriptes ausführlich hergeleitet wird. Das bedeutet, wir müssen ein Problem folgender Art lösen.
	\begin{equation}
		Ax = b
	\end{equation}
	Es sei dabei $A$ die Koeffizientenmatrix, $b$ der Störvektor und $x$ der Lösungsvektor. 
	
	Die leitenden Platten $b$ und $x$ werden als eine Fläche visualisiert. Um diese Als Vektor zu schreiben ist dabei festgelegt worden, dass die Spalten dieser diskretisierten Fläche untereinander geschrieben werden. Dies ergibt Vektoren der länge $n^2$.
	
	Aus diesem Zusammenhang heraus ergibt sich dabei die Koeffizientenmatrix $A$. 
\[
	A=\left(
	\begin{array}{ccccc|ccccc|c|ccccc}
	    -4&     1&     0&\cdots&     0 &     1&     0&     0&\cdots&     0 &\cdots &      &      &      &      &      \\
	     1&    -4&     1&\cdots&     0 &     0&     1&     0&\cdots&     0 &\cdots &      &      &      &      &      \\
	     0&     1&    -4&\cdots&     0 &     0&     0&     1&\cdots&     0 &\cdots &      &      &     0&      &      \\
	\vdots&\vdots&\vdots&\ddots&\vdots &\vdots&\vdots&\vdots&\ddots&\vdots &       &      &      &      &      &      \\
	     0&     0&     0&\cdots&    -4 &     0&     0&     0&\dots &     1 &\cdots &      &      &      &      &      \\
	\hline
	     1&     0&     0&\cdots&     0 &    -4&     1&     0&\dots &     0 &\cdots &      &      &      &      &      \\
	     0&     1&     0&\cdots&     0 &     1&    -4&     1&\dots &     0 &\cdots &      &      &      &      &      \\
	     0&     0&     1&\cdots&     0 &     0&     1&    -4&\dots &     0 &\cdots &      &      &     0&      &      \\
	\vdots&\vdots&\vdots&\ddots&\vdots &\vdots&\vdots&\vdots&\ddots&\vdots &       &      &      &      &      &      \\
	     0&     0&     0&\cdots&     1 &     0&     0&     0&\cdots&    -4 &\cdots &      &      &      &      &      \\
	\hline
	\vdots&\vdots&\vdots&      &\vdots &\vdots&\vdots&\vdots&      &\vdots &\ddots &\vdots&\vdots&\vdots&      &\vdots\\
	\hline
	      &      &      &      &       &      &      &      &      &       &\cdots &    -4&     1&     0&\cdots&     0\\
	      &      &      &      &       &      &      &      &      &       &\cdots &     1&    -4&     1&\cdots&     0\\
	      &      &     0&      &       &      &      &     0&      &       &\cdots &     0&     1&    -4&\cdots&     0\\
	      &      &      &      &       &      &      &      &      &       &       &\vdots&\vdots&\vdots&\ddots&\vdots\\
	      &      &      &      &       &      &      &      &      &       &\cdots &     0&     0&     0&\cdots&    -4\\
	\end{array}
	\right) 
	\]

\subsection{Technische Umsetzung}
	Die Matrix $A$ enthält die Koeffizienten für die partielle Differentialgleichung der zweiten Ordnung. Sie hat die Grösse $n^2 \times n^2$. Diese Matrix auf dem Heap zu allozieren wäre für mittelgrosse Matrizen schon nicht mehr möglich.\cite{mueller:hpcseminar}
	
	Der benötigte Speicher für eine Matrix $f$ mit der Grösse $500 \times 500$ und \verb|float| Werten (4 Byte) wäre
	
	\begin{equation}
		500^2 \times 500^2 \cdot 4\;\mathrm{Byte} = 232.8\;\mathrm{GByte}
	\end{equation}
	
	Mit $n = 1000$ sogar 3.64\;TByte, also definitiv zu viel. Dies ist aber auch gar nicht nötig. Es müssen jeweils nur die Nachbarelemente von $f_k$ addiert und durch einen konstanten Faktor geteilt werden, die Randelemente sind dabei Null zu setzen.

	\begin{eqnarray}
		f_k = U_{k-1}+U_{k+1}+U_{k-(n-1)}+U_{k+(n-1)}-4U_k
	\end{eqnarray}
	
	Da die diskrete partielle Ableitung jeweils nur die Elemente ober/unterhalb und links/recht des aktuellen Elementes in die Rechnung mit einbezieht, müssen mindestens $n$ Iterationen durchgeführt werden, damit sich das Potential über die ganze Ebene verteilt.
	
	\subsubsection{Parallelisierung}
	Um das Gleichungssystem (\ref{eq:gleichung}) zu lösen, haben wir den Gauss-Seidel Algorithmus benutzt. Bei diesem Algorithmus ist, wie wir wissen, die aktuelle Zeile von der vorherigen abhängig. Bei der Parallelisierung wird das Gleichungssystem an verschiedenen Stellen zu lösen begonnen. Das ist zwar nicht so effizient, wie ein "'normaler"' Gauss-Seidel, wird aber durch die Parallelisierung schneller.
		
		Die Abweichung ist bei den ersten Schritten am grössten, und wird bei jeder Iteration kleiner. Als wie die ersten Berechnungen durch geführt hatten, fiel uns auf, dass die einzelnen Threads am Anfang gut als Linien von Spitzen sichtbar sind (\fref{fig:201_1}).
		
		\begin{figure}[h]
			\centering
			\includegraphics[width = 15cm]{./images/step001}
			\caption{Berechnung von einer $201 \times 201$ Fläche mit 32 Threads nach dem ersten Iterationsschritt}
			\label{fig:201_1}
		\end{figure}
		
		
		Wir haben uns für Open-MP entschieden, da es für uns die einfachste Methode war, die Berechnung zu parallelisieren. Wie bei der Arbeit von Christen und Solenthaler erklärt, wäre Open-MPI die effizienteste Weise, um solch ein Problem zu lösen. Da hier nur die Resultate des Algorithmus von Interesse waren und wir relativ kleine Probleme gelöst haben, hat es sich nicht gelohnt, den Mehraufwand für Open-MPI zu betreiben. Die Rechenzeit war absolut im Rahmen.
		
		Wie schon beim Kugelsternhaufen erwähnt, ist nun diese Open-MP Parallelisierung einfach zu realisieren, wie folgendes Beispiel schön zeigt. 
		
\begin{code}
	int numthreads = 32;
	#pragma omp parallel for num_threads(numthreads)
		for (i = 0; i < dim; i++)
		{ ...
\end{code}
	\subsubsection{Dateiformate}
	Um mit der Datenflut umgehen zu können, die entsteht, wenn man mit solchen Problemen zu tun hat, musste ebenfalls eine Lösung entwickelt werden. Als Eingabedatei wurde ein Bild im FITS Format verwendet. Da es jedoch einfacher schien wurde während des Programmes mit CSV Files gearbeitet. Dabei war der technische Aufwand gering und die Resultate konnten schnell kontrolliert werden. Aus diesen CSV Dateien kann mit GNU-Plot ein Bild erstellt werden. Optional dienen diese als Vorlage für einen kurzen Film. 
	
	Da FITS ein für uns unbekanntes Dateiformat war, haben wir uns entschieden, dieses kurz zu erläutern.	

	Flexible Image Transport System ist die ausgeschriebene Bezeichnung für das Wort FITS. Es ist ein quelloffenes Dateiformat, womit Daten verlustlos abgespeichert werden können. Die Nasa entwickelte es und in der Astronomie hat es sich als Standardformat etabliert.  Informationen können in verschiedenen Layern abgelegt werden. Das bedeutet, ein Gitterpunkt in einem Datenfeld kann unterschiedliche Informationen enthalten. Angenehm ist ausserdem, dass die Werte direkt in Flisskommadarstellung abgespeichert werden können \cite{nasa:fits}. Angenehm ist, dass mit vielen gängigen Bildbearbeitungsprogrammen dieses Dateiformat angeschaut werden kann.
		
 
		
		

